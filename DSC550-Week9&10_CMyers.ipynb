{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# #1\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# #2\n",
    "import keras\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# #3\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the jsonlines file and then fill a dataframe with the jsonlines data.\n",
    "# Due to the computation time down stream, only 50k rows will be used.\n",
    "df = pd.read_json(\"categorized-comments.jsonl\", lines = True, nrows = 50000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that cleans (lowercase transformation, removal of special chars, etc.) our txt field row by \n",
    "# row using three regular expressions\n",
    "\n",
    "def clean_data(data):\n",
    "    data=data.lower()\n",
    "    data=re.sub('&lt;/?.*?&gt;',' &lt;&gt', data)\n",
    "    data=re.sub('\\\\d|\\\\W+|_',' ', data)\n",
    "    data=re.sub('[^a-zA-Z]',\" \", data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of english stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>barely better than gabbert he was significantl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>fuck the ducks and the angels but welcome to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>should have drafted more wrs matt millen probably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>done https i imgur com  yz  pm jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>no noo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  barely better than gabbert he was significantl...\n",
       "1  sports  fuck the ducks and the angels but welcome to a...\n",
       "2  sports  should have drafted more wrs matt millen probably\n",
       "3  sports                done https i imgur com  yz  pm jpg \n",
       "4  sports                                            no noo "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean our txt field using our previously defined function\n",
    "df['txt'] = df['txt'].apply(lambda x:clean_data(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#1 \\n\\nFit a neural network classifier using sklearn and report the model accuracy, \\npercision, f1 score, and the confusion matrix.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#1 \n",
    "\n",
    "Fit a neural network classifier using sklearn and report the model accuracy, \n",
    "percision, f1 score, and the confusion matrix.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a count vectorizer while ignoring the stopwords\n",
    "cv = CountVectorizer(stop_words = stopwords)\n",
    "\n",
    "# Fit our count vectorizer with our txt field and save it as our input variable for our neural network classifier\n",
    "X = cv.fit_transform(df['txt'])\n",
    "Y = df['cat']\n",
    "\n",
    "# Split our data set for training and testing purposes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our multi-layer perceptron classifier with 100 hidden layers\n",
    "mlp = MLPClassifier(hidden_layer_sizes=10, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54078378\n",
      "Iteration 2, loss = 0.33902826\n",
      "Iteration 3, loss = 0.26358546\n",
      "Iteration 4, loss = 0.22303336\n",
      "Iteration 5, loss = 0.19672773\n",
      "Iteration 6, loss = 0.17768878\n",
      "Iteration 7, loss = 0.16306179\n",
      "Iteration 8, loss = 0.15143619\n",
      "Iteration 9, loss = 0.14202464\n",
      "Iteration 10, loss = 0.13419728\n",
      "Iteration 11, loss = 0.12773423\n",
      "Iteration 12, loss = 0.12206232\n",
      "Iteration 13, loss = 0.11728928\n",
      "Iteration 14, loss = 0.11299526\n",
      "Iteration 15, loss = 0.10943814\n",
      "Iteration 16, loss = 0.10618278\n",
      "Iteration 17, loss = 0.10348423\n",
      "Iteration 18, loss = 0.10089233\n",
      "Iteration 19, loss = 0.09874630\n",
      "Iteration 20, loss = 0.09672449\n",
      "Iteration 21, loss = 0.09493408\n",
      "Iteration 22, loss = 0.09331358\n",
      "Iteration 23, loss = 0.09187328\n",
      "Iteration 24, loss = 0.09053142\n",
      "Iteration 25, loss = 0.08930152\n",
      "Iteration 26, loss = 0.08815980\n",
      "Iteration 27, loss = 0.08711917\n",
      "Iteration 28, loss = 0.08627593\n",
      "Iteration 29, loss = 0.08546292\n",
      "Iteration 30, loss = 0.08459402\n",
      "Iteration 31, loss = 0.08381975\n",
      "Iteration 32, loss = 0.08318751\n",
      "Iteration 33, loss = 0.08262334\n",
      "Iteration 34, loss = 0.08201950\n",
      "Iteration 35, loss = 0.08150611\n",
      "Iteration 36, loss = 0.08092848\n",
      "Iteration 37, loss = 0.08061475\n",
      "Iteration 38, loss = 0.08007593\n",
      "Iteration 39, loss = 0.07969462\n",
      "Iteration 40, loss = 0.07933340\n",
      "Iteration 41, loss = 0.07902628\n",
      "Iteration 42, loss = 0.07865451\n",
      "Iteration 43, loss = 0.07834199\n",
      "Iteration 44, loss = 0.07810030\n",
      "Iteration 45, loss = 0.07769766\n",
      "Iteration 46, loss = 0.07748952\n",
      "Iteration 47, loss = 0.07725014\n",
      "Iteration 48, loss = 0.07703751\n",
      "Iteration 49, loss = 0.07691651\n",
      "Iteration 50, loss = 0.07655244\n",
      "Iteration 51, loss = 0.07653650\n",
      "Iteration 52, loss = 0.07623857\n",
      "Iteration 53, loss = 0.07597055\n",
      "Iteration 54, loss = 0.07577999\n",
      "Iteration 55, loss = 0.07569399\n",
      "Iteration 56, loss = 0.07557759\n",
      "Iteration 57, loss = 0.07550361\n",
      "Iteration 58, loss = 0.07534034\n",
      "Iteration 59, loss = 0.07529333\n",
      "Iteration 60, loss = 0.07500733\n",
      "Iteration 61, loss = 0.07499787\n",
      "Iteration 62, loss = 0.07486329\n",
      "Iteration 63, loss = 0.07472830\n",
      "Iteration 64, loss = 0.07459659\n",
      "Iteration 65, loss = 0.07458951\n",
      "Iteration 66, loss = 0.07449777\n",
      "Iteration 67, loss = 0.07442693\n",
      "Iteration 68, loss = 0.07436301\n",
      "Iteration 69, loss = 0.07428741\n",
      "Iteration 70, loss = 0.07411432\n",
      "Iteration 71, loss = 0.07413865\n",
      "Iteration 72, loss = 0.07405939\n",
      "Iteration 73, loss = 0.07395806\n",
      "Iteration 74, loss = 0.07393327\n",
      "Iteration 75, loss = 0.07394342\n",
      "Iteration 76, loss = 0.07375458\n",
      "Iteration 77, loss = 0.07395648\n",
      "Iteration 78, loss = 0.07373671\n",
      "Iteration 79, loss = 0.07363537\n",
      "Iteration 80, loss = 0.07363951\n",
      "Iteration 81, loss = 0.07356838\n",
      "Iteration 82, loss = 0.07355318\n",
      "Iteration 83, loss = 0.07352315\n",
      "Iteration 84, loss = 0.07342154\n",
      "Iteration 85, loss = 0.07342225\n",
      "Iteration 86, loss = 0.07339086\n",
      "Iteration 87, loss = 0.07350578\n",
      "Iteration 88, loss = 0.07340280\n",
      "Iteration 89, loss = 0.07333109\n",
      "Iteration 90, loss = 0.07322840\n",
      "Iteration 91, loss = 0.07319203\n",
      "Iteration 92, loss = 0.07331670\n",
      "Iteration 93, loss = 0.07312433\n",
      "Iteration 94, loss = 0.07324587\n",
      "Iteration 95, loss = 0.07323684\n",
      "Iteration 96, loss = 0.07324206\n",
      "Iteration 97, loss = 0.07312077\n",
      "Iteration 98, loss = 0.07303229\n",
      "Iteration 99, loss = 0.07311806\n",
      "Iteration 100, loss = 0.07312391\n",
      "Iteration 101, loss = 0.07297472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=10, verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our neural network classifier with our training data\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of predictions using our testing data\n",
    "preds = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: \n",
      " 0.8388\n"
     ]
    }
   ],
   "source": [
    "print('Model Accuracy: \\n', accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[8700 1424]\n",
      " [1800 8076]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.83      0.86      0.84     10124\n",
      "                sports       0.85      0.82      0.83      9876\n",
      "\n",
      "              accuracy                           0.84     20000\n",
      "             macro avg       0.84      0.84      0.84     20000\n",
      "          weighted avg       0.84      0.84      0.84     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report: \\n', classification_report(y_test, preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory of all large variables\n",
    "del df\n",
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#2\\n\\nFit a neural network classifier using keras and report the model accuracy, \\npercision, f1 score, and the confusion matrix.\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#2\n",
    "\n",
    "Fit a neural network classifier using keras and report the model accuracy, \n",
    "percision, f1 score, and the confusion matrix.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the categorized comments into a dataframe\n",
    "\n",
    "df = pd.read_json(\"categorized-comments.jsonl\", lines = True, nrows = 100000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>does this work if you use gravitybox to flip t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>would be also cool if they came without bootlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>but it does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>i love allo s stickers maybe allo could be the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>gt as you know nougat has been out for a dece...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      cat                                                txt\n",
       "0  science_and_technology  does this work if you use gravitybox to flip t...\n",
       "1  science_and_technology  would be also cool if they came without bootlo...\n",
       "2  science_and_technology                                       but it does \n",
       "3  science_and_technology  i love allo s stickers maybe allo could be the...\n",
       "4  science_and_technology   gt as you know nougat has been out for a dece..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the sample size\n",
    "size = 1500\n",
    "\n",
    "# Define the replace argument in our lambda function\n",
    "replace = True\n",
    "\n",
    "# Define the lambda function that randomly chooses 1500 of each category for our resulting dataframe\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "\n",
    "# Group each of our categories in a new dataset and apply our previously defined lambda function\n",
    "cat_df = df.groupby('cat', as_index=False).apply(fn)\n",
    "\n",
    "# Remove all punctuation, make all letters lowercase,and remove special characters\n",
    "cat_df['txt'] = cat_df['txt'].apply(lambda x:clean_data(x))\n",
    "\n",
    "# Reset the index of our cleaned dataframe\n",
    "cat_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat\n",
       "science_and_technology    1500\n",
       "sports                    1500\n",
       "video_games               1500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the categories have equal row counts\n",
    "cat_df.groupby([\"cat\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LabelEncoder\n",
    "# The label encoder is a type of encoding that should only be used on the target values\n",
    "# This replaces our categorical data with numeric data that ranges from 0 to n - 1.\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Define our target variable\n",
    "cat = cat_df[\"cat\"]\n",
    "\n",
    "# Fit our encoder and return a new row of numeric data\n",
    "cat_df[\"cat\"] = enc.fit_transform(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat\n",
       "0    1500\n",
       "1    1500\n",
       "2    1500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the encoding worked as expected\n",
    "# We are expecting the same dataframe that we saw the previous time we ran this code, except that the categories are numeric\n",
    "cat_df.groupby([\"cat\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the max number of features our count vectorizer will use\n",
    "feat_count = 1500\n",
    "\n",
    "# Instantiate the count vectorizer, which we will in turn use to create a feature matrix\n",
    "cv = CountVectorizer(analyzer='word',\n",
    "                     stop_words=stopwords, \n",
    "                     max_features = feat_count,\n",
    "                     max_df = 0.5,\n",
    "                     min_df = 3)\n",
    "\n",
    "# Define a feature matrix based on our txt field\n",
    "X = cv.fit_transform(cat_df['txt'])\n",
    "\n",
    "# Define our target variable\n",
    "y = cat_df['cat']\n",
    "\n",
    "# Split our dataset for training and testing purposes using 40% of our data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sparse matrix to a sparse tensor so that the sequential classifier can accept it\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    # Create a coordinate representation of our scipy sparse matrix\n",
    "    coo = X.tocoo()\n",
    "    \n",
    "    # Define the indicies of the coordinate matrix and transpose the matrix\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    \n",
    "    # Return a sparse tensor\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our scipy sparse matrix to a sparse tensor and then reorder the indicies\n",
    "X_train = tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas series to a tensor\n",
    "y_train = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our sequential classifier\n",
    "classifier_seq = Sequential()\n",
    "\n",
    "# Add layers to our sequential classifier\n",
    "classifier_seq.add(Dense(units=500,activation=\"relu\",input_shape=(feat_count,)))\n",
    "classifier_seq.add(Dense(units=50, activation=\"relu\"))\n",
    "classifier_seq.add(Dense(units=4, activation=\"softmax\"))\n",
    "\n",
    "# Compile the sequential artificial neural network\n",
    "classifier_seq.compile(optimizer=\"rmsprop\", \n",
    "                       loss=\"sparse_categorical_crossentropy\", \n",
    "                       metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.1903 - accuracy: 0.4724\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7111 - accuracy: 0.7913\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5159 - accuracy: 0.8269\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3847 - accuracy: 0.8583\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3181 - accuracy: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ac9482ba00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the training data and expose the progress of the fit with the verbose argument\n",
    "classifier_seq.fit(X_train, y_train, batch_size=200, epochs=5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               750500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 775,754\n",
      "Trainable params: 775,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Observe a summary of the layers of our neural network\n",
    "classifier_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our scipy sparse matrix to a sparse tensor and then reorder the indicies\n",
    "X_test = tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas series to a tensor\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 652us/step - loss: 0.6153 - accuracy: 0.7228\n",
      "Training Accuracy: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and define loss and accuracy\n",
    "loss, accuracy = classifier_seq.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Training Accuracy: \\n\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# Define the target predictions based upon on the test set of our inputs\n",
    "y_pred = classifier_seq.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[454 129  30]\n",
      " [ 68 460  44]\n",
      " [ 63 165 387]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76       613\n",
      "           1       0.61      0.80      0.69       572\n",
      "           2       0.84      0.63      0.72       615\n",
      "\n",
      "    accuracy                           0.72      1800\n",
      "   macro avg       0.74      0.72      0.72      1800\n",
      "weighted avg       0.74      0.72      0.72      1800\n",
      "\n",
      "Accuracy: \n",
      " 0.7227777777777777\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics of our model\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,y_pred))\n",
    "print(\"Accuracy: \\n\", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory of our larger variables\n",
    "del df\n",
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#3\\n\\nClassify MSINT images using convolutional neural network and report the accuracy of the results.\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#3\n",
    "\n",
    "Classify MSINT images using convolutional neural network and report the accuracy of the results.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color channel value \n",
    "K.set_image_data_format(\"channels_first\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training data image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "# Reshape the training data image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start sequential neural network \n",
    "network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters = 64,\n",
    "                  kernel_size = (5,5),\n",
    "                  input_shape = (channels, width, height),\n",
    "                  activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add max pooling layer with a 2x2 layer window\n",
    "network.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a layer to flatten the input\n",
    "network.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(10, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the sequential neural network\n",
    "network.compile(loss = \"categorical_crossentropy\",\n",
    "               optimizer = \"rmsprop\",\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " Default MaxPoolingOp only supports NHWC on device type CPU\n\t [[node sequential_1/max_pooling2d/MaxPool (defined at <ipython-input-57-6a17f0cd7ec4>:2) ]] [Op:__inference_train_function_2338]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-6a17f0cd7ec4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the sequential neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m network.fit(features_train,\n\u001b[0m\u001b[0;32m      3\u001b[0m            \u001b[0mtarget_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m            \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m            \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Default MaxPoolingOp only supports NHWC on device type CPU\n\t [[node sequential_1/max_pooling2d/MaxPool (defined at <ipython-input-57-6a17f0cd7ec4>:2) ]] [Op:__inference_train_function_2338]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Train the sequential neural network\n",
    "network.fit(features_train,\n",
    "           target_train,\n",
    "           epochs = 2,\n",
    "           verbose = 0,\n",
    "           batch_size = 1000,\n",
    "           validation_data = (features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
